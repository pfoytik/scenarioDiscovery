{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "import dash\n",
    "from dash import dcc, html, Input, Output, State\n",
    "import plotly.graph_objs as go\n",
    "import plotly.express as px\n",
    "import joblib\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_in_box(row, box_min, box_max):\n",
    "    for feat, min_val in box_min.items():\n",
    "        if row.get(int(feat), 0) < min_val:\n",
    "            return False\n",
    "    for feat, max_val in box_max.items():\n",
    "        if row.get(int(feat), 0) > max_val:\n",
    "            return False\n",
    "    return True\n",
    "\n",
    "def generate_box_features(X: pd.DataFrame, box_data: dict) -> pd.DataFrame:\n",
    "    box_features = pd.DataFrame(index=X.index)\n",
    "\n",
    "    for box_id, box_limits in box_data.items():\n",
    "        box_min = box_limits.get('min', {})\n",
    "        box_max = box_limits.get('max', {})\n",
    "\n",
    "        # For each row, determine if it satisfies the box conditions\n",
    "        box_features[f'box_{box_id}'] = X.apply(lambda row: is_in_box(row, box_min, box_max), axis=1).astype(int)\n",
    "\n",
    "    return box_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom model as fallback\n",
    "class CustomModel:\n",
    "    def __init__(self, road_importances):\n",
    "        self.road_importances = road_importances\n",
    "    \n",
    "    def predict(self, X):\n",
    "        X_array = X[0]  # Get the input array\n",
    "        selected_indices = np.where(X_array > 0)[0]\n",
    "        selected_roads = [idx for idx in selected_indices]\n",
    "        \n",
    "        # Base time + penalty for each closed road\n",
    "        base_time = 3.0\n",
    "        road_penalty = sum(self.road_importances.get(idx, 0.01) * 50 \n",
    "                          for idx in selected_indices)\n",
    "        \n",
    "        return np.array([base_time + road_penalty])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build meta-model\n",
    "def build_meta_model(df):\n",
    "    # Separate features and target\n",
    "    X = df.drop('evacuation_time', axis=1)\n",
    "    y = df['evacuation_time']\n",
    "    \n",
    "    # Check if there's enough variation in the target\n",
    "    if y.std() < 0.1:\n",
    "        print(\"WARNING: Not enough variation in evacuation times. Adding synthetic data...\")\n",
    "        # Create some synthetic data points\n",
    "        for i in range(20):\n",
    "            # Generate random road closures\n",
    "            row = np.zeros(len(X.columns))\n",
    "            num_closed = np.random.randint(2, 10)\n",
    "            closed_indices = np.random.choice(len(X.columns), num_closed, replace=False)\n",
    "            row[closed_indices] = 1\n",
    "            \n",
    "            # Generate reasonable evacuation time (10-60 minutes)\n",
    "            evacuation_time = 10 + np.random.rand() * 50\n",
    "            \n",
    "            # Add to training data\n",
    "            X = pd.concat([X, pd.DataFrame([row], columns=X.columns)])\n",
    "            y = pd.concat([y, pd.Series([evacuation_time])])\n",
    "\n",
    "    # Train model\n",
    "    model = RandomForestRegressor(n_estimators=100, min_samples_leaf=2, random_state=42)\n",
    "    model.fit(X, y)\n",
    "    \n",
    "    # Verify model works by testing different inputs\n",
    "    test_input = np.zeros(X.shape[1])\n",
    "    baseline = model.predict([test_input])[0]\n",
    "    \n",
    "    test_input[0] = 1  # Close first road\n",
    "    pred1 = model.predict([test_input])[0]\n",
    "    \n",
    "    test_input[0] = 0\n",
    "    test_input[10] = 1  # Close a different road\n",
    "    pred2 = model.predict([test_input])[0]\n",
    "    \n",
    "    print(f\"Model test - Baseline: {baseline}, Pred1: {pred1}, Pred2: {pred2}\")\n",
    "    #if abs(baseline - pred1) < 0.01 and abs(baseline - pred2) < 0.01:\n",
    "    #    print(\"WARNING: Model not responding to different inputs. Returning a custom model...\")\n",
    "    #    return CustomModel(road_importances)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[16], line 53\u001b[0m\n\u001b[1;32m     50\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m df\n\u001b[1;32m     52\u001b[0m \u001b[38;5;66;03m# Generate sample data\u001b[39;00m\n\u001b[0;32m---> 53\u001b[0m scenarios_df \u001b[38;5;241m=\u001b[39m \u001b[43mgenerate_sample_scenarios\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m500\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m2200\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     55\u001b[0m \u001b[38;5;66;03m# Save to CSV (saving just a subset of columns for demonstration)\u001b[39;00m\n\u001b[1;32m     56\u001b[0m \u001b[38;5;66;03m# In reality, you'd save all columns\u001b[39;00m\n\u001b[1;32m     57\u001b[0m sample_df \u001b[38;5;241m=\u001b[39m scenarios_df\u001b[38;5;241m.\u001b[39miloc[:, \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m50\u001b[39m)) \u001b[38;5;241m+\u001b[39m [\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]]\n",
      "Cell \u001b[0;32mIn[16], line 48\u001b[0m, in \u001b[0;36mgenerate_sample_scenarios\u001b[0;34m(num_scenarios, num_roads)\u001b[0m\n\u001b[1;32m     46\u001b[0m     \u001b[38;5;66;03m# Create row\u001b[39;00m\n\u001b[1;32m     47\u001b[0m     row \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(road_closures) \u001b[38;5;241m+\u001b[39m [evacuation_time]\n\u001b[0;32m---> 48\u001b[0m     \u001b[43mdf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mloc\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;241m=\u001b[39m row\n\u001b[1;32m     50\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m df\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/pandas/core/indexing.py:849\u001b[0m, in \u001b[0;36m_LocationIndexer.__setitem__\u001b[0;34m(self, key, value)\u001b[0m\n\u001b[1;32m    846\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_has_valid_setitem_indexer(key)\n\u001b[1;32m    848\u001b[0m iloc \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mname \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124miloc\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobj\u001b[38;5;241m.\u001b[39miloc\n\u001b[0;32m--> 849\u001b[0m \u001b[43miloc\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_setitem_with_indexer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindexer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/pandas/core/indexing.py:1825\u001b[0m, in \u001b[0;36m_iLocIndexer._setitem_with_indexer\u001b[0;34m(self, indexer, value, name)\u001b[0m\n\u001b[1;32m   1822\u001b[0m     indexer, missing \u001b[38;5;241m=\u001b[39m convert_missing_indexer(indexer)\n\u001b[1;32m   1824\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m missing:\n\u001b[0;32m-> 1825\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_setitem_with_indexer_missing\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindexer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1826\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[1;32m   1828\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mloc\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m   1829\u001b[0m     \u001b[38;5;66;03m# must come after setting of missing\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/pandas/core/indexing.py:2180\u001b[0m, in \u001b[0;36m_iLocIndexer._setitem_with_indexer_missing\u001b[0;34m(self, indexer, value)\u001b[0m\n\u001b[1;32m   2178\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobj\u001b[38;5;241m.\u001b[39m_mgr \u001b[38;5;241m=\u001b[39m df\u001b[38;5;241m.\u001b[39m_mgr\n\u001b[1;32m   2179\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 2180\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobj\u001b[38;5;241m.\u001b[39m_mgr \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mobj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_append\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39m_mgr\n\u001b[1;32m   2181\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobj\u001b[38;5;241m.\u001b[39m_maybe_update_cacher(clear\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/pandas/core/frame.py:9558\u001b[0m, in \u001b[0;36mDataFrame._append\u001b[0;34m(self, other, ignore_index, verify_integrity, sort)\u001b[0m\n\u001b[1;32m   9555\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   9556\u001b[0m     to_concat \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m, other]\n\u001b[0;32m-> 9558\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[43mconcat\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   9559\u001b[0m \u001b[43m    \u001b[49m\u001b[43mto_concat\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   9560\u001b[0m \u001b[43m    \u001b[49m\u001b[43mignore_index\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_index\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   9561\u001b[0m \u001b[43m    \u001b[49m\u001b[43mverify_integrity\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mverify_integrity\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   9562\u001b[0m \u001b[43m    \u001b[49m\u001b[43msort\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msort\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   9563\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   9564\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m result\u001b[38;5;241m.\u001b[39m__finalize__(\u001b[38;5;28mself\u001b[39m, method\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mappend\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/pandas/core/reshape/concat.py:385\u001b[0m, in \u001b[0;36mconcat\u001b[0;34m(objs, axis, join, ignore_index, keys, levels, names, verify_integrity, sort, copy)\u001b[0m\n\u001b[1;32m    370\u001b[0m     copy \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m    372\u001b[0m op \u001b[38;5;241m=\u001b[39m _Concatenator(\n\u001b[1;32m    373\u001b[0m     objs,\n\u001b[1;32m    374\u001b[0m     axis\u001b[38;5;241m=\u001b[39maxis,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    382\u001b[0m     sort\u001b[38;5;241m=\u001b[39msort,\n\u001b[1;32m    383\u001b[0m )\n\u001b[0;32m--> 385\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mop\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_result\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/pandas/core/reshape/concat.py:616\u001b[0m, in \u001b[0;36m_Concatenator.get_result\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    612\u001b[0m             indexers[ax] \u001b[38;5;241m=\u001b[39m obj_labels\u001b[38;5;241m.\u001b[39mget_indexer(new_labels)\n\u001b[1;32m    614\u001b[0m     mgrs_indexers\u001b[38;5;241m.\u001b[39mappend((obj\u001b[38;5;241m.\u001b[39m_mgr, indexers))\n\u001b[0;32m--> 616\u001b[0m new_data \u001b[38;5;241m=\u001b[39m \u001b[43mconcatenate_managers\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    617\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmgrs_indexers\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnew_axes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconcat_axis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbm_axis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcopy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcopy\u001b[49m\n\u001b[1;32m    618\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    619\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcopy \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m using_copy_on_write():\n\u001b[1;32m    620\u001b[0m     new_data\u001b[38;5;241m.\u001b[39m_consolidate_inplace()\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/pandas/core/internals/concat.py:232\u001b[0m, in \u001b[0;36mconcatenate_managers\u001b[0;34m(mgrs_indexers, axes, concat_axis, copy)\u001b[0m\n\u001b[1;32m    226\u001b[0m vals \u001b[38;5;241m=\u001b[39m [ju\u001b[38;5;241m.\u001b[39mblock\u001b[38;5;241m.\u001b[39mvalues \u001b[38;5;28;01mfor\u001b[39;00m ju \u001b[38;5;129;01min\u001b[39;00m join_units]\n\u001b[1;32m    228\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m blk\u001b[38;5;241m.\u001b[39mis_extension:\n\u001b[1;32m    229\u001b[0m     \u001b[38;5;66;03m# _is_uniform_join_units ensures a single dtype, so\u001b[39;00m\n\u001b[1;32m    230\u001b[0m     \u001b[38;5;66;03m#  we can use np.concatenate, which is more performant\u001b[39;00m\n\u001b[1;32m    231\u001b[0m     \u001b[38;5;66;03m#  than concat_compat\u001b[39;00m\n\u001b[0;32m--> 232\u001b[0m     values \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconcatenate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvals\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    233\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    234\u001b[0m     \u001b[38;5;66;03m# TODO(EA2D): special-casing not needed with 2D EAs\u001b[39;00m\n\u001b[1;32m    235\u001b[0m     values \u001b[38;5;241m=\u001b[39m concat_compat(vals, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "# Create sample data\n",
    "def generate_sample_scenarios(num_scenarios=500, num_roads=50):\n",
    "    # Initialize dataframe columns\n",
    "    columns = [f'road_{i}' for i in range(num_roads)]\n",
    "    columns.append('evacuation_time')\n",
    "    \n",
    "    # Create empty dataframe\n",
    "    df = pd.DataFrame(columns=columns)\n",
    "    \n",
    "    # Generate scenarios\n",
    "    for i in range(num_scenarios):\n",
    "        # For each scenario, randomly close 5-20 roads\n",
    "        num_closed_roads = random.randint(5, 20)\n",
    "        road_closures = np.zeros(num_roads)\n",
    "        \n",
    "        # Randomly select roads to close\n",
    "        closed_road_indices = random.sample(range(num_roads), num_closed_roads)\n",
    "        road_closures[closed_road_indices] = 1\n",
    "        \n",
    "        # Simple model for evacuation time:\n",
    "        # - Base time: 60 minutes\n",
    "        # - Each closed road adds 2-10 minutes depending on importance\n",
    "        # - Some combinations have extra penalties\n",
    "        road_importance = np.random.exponential(scale=1.0, size=num_roads)\n",
    "        road_importance = road_importance / np.max(road_importance)  # Normalize to 0-1\n",
    "        \n",
    "        base_time = 60\n",
    "        road_effect = np.sum(road_closures * road_importance * 10)\n",
    "        \n",
    "        # Add some interaction effects for certain combinations\n",
    "        key_road_pairs = [(10, 25), (50, 100), (200, 300), (400, 500)]\n",
    "        interaction_penalty = 0\n",
    "        for r1, r2 in key_road_pairs:\n",
    "            if road_closures[r1] == 1 and road_closures[r2] == 1:\n",
    "                interaction_penalty += 15\n",
    "        \n",
    "        # Add noise\n",
    "        noise = np.random.normal(0, 5)\n",
    "        \n",
    "        evacuation_time = base_time + road_effect + interaction_penalty + noise\n",
    "        \n",
    "        # Create row\n",
    "        row = list(road_closures) + [evacuation_time]\n",
    "        df.loc[i] = row\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Generate sample data\n",
    "scenarios_df = generate_sample_scenarios(500, 2200)\n",
    "\n",
    "# Save to CSV (saving just a subset of columns for demonstration)\n",
    "# In reality, you'd save all columns\n",
    "sample_df = scenarios_df.iloc[:, list(range(0, 50)) + [-1]]\n",
    "sample_df.to_csv('evacuation_scenarios_sample.csv', index=False)\n",
    "\n",
    "print(\"Sample data generated and saved to 'evacuation_scenarios_sample.csv'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature importance file generated and saved to 'road_importance.csv'\n"
     ]
    }
   ],
   "source": [
    "def generate_feature_importance_file(num_roads=50):\n",
    "    # Generate mock feature importance values\n",
    "    # In real life, these would come from your model analysis\n",
    "    importances = np.random.exponential(scale=1.0, size=num_roads)\n",
    "    importances = importances / np.sum(importances)  # Normalize to sum to 1\n",
    "    \n",
    "    # Create dataframe\n",
    "    importance_df = pd.DataFrame({\n",
    "        'road_id': [f'road_{i}' for i in range(num_roads)],\n",
    "        'importance': importances\n",
    "    })\n",
    "    \n",
    "    # Sort by importance\n",
    "    importance_df = importance_df.sort_values('importance', ascending=False)\n",
    "    \n",
    "    # Save to CSV\n",
    "    importance_df.to_csv('road_importance.csv', index=False)\n",
    "    \n",
    "    print(\"Feature importance file generated and saved to 'road_importance.csv'\")\n",
    "\n",
    "generate_feature_importance_file()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scenario discovery results saved to 'scenario_discovery_results.json'\n"
     ]
    }
   ],
   "source": [
    "def generate_scenario_discovery_results():\n",
    "    # This would represent the output from your scenario discovery analysis\n",
    "    # Format: combinations of roads and their impact on evacuation time\n",
    "    \n",
    "    scenarios = [\n",
    "        {\n",
    "            'scenario_id': 1,\n",
    "            'description': 'Major arterial routes blocked',\n",
    "            'conditions': [\n",
    "                {'road_id': 'road_10', 'status': 'closed'},\n",
    "                {'road_id': 'road_25', 'status': 'closed'},\n",
    "                {'road_id': 'road_42', 'status': 'closed'}\n",
    "            ],\n",
    "            'avg_evacuation_time': 112.5,\n",
    "            'probability_high_impact': 0.87,\n",
    "            'coverage': 0.12  # Portion of high-impact scenarios covered by this rule\n",
    "        },\n",
    "        {\n",
    "            'scenario_id': 2,\n",
    "            'description': 'Southern exit routes blocked',\n",
    "            'conditions': [\n",
    "                {'road_id': 'road_10', 'status': 'closed'},\n",
    "                {'road_id': 'road_16', 'status': 'closed'},\n",
    "                {'road_id': 'road_17', 'status': 'closed'}\n",
    "            ],\n",
    "            'avg_evacuation_time': 98.3,\n",
    "            'probability_high_impact': 0.75,\n",
    "            'coverage': 0.08\n",
    "        },\n",
    "        {\n",
    "            'scenario_id': 3,\n",
    "            'description': 'Downtown grid blocked',\n",
    "            'conditions': [\n",
    "                {'road_id': 'road_25', 'status': 'closed'},\n",
    "                {'road_id': 'road_21', 'status': 'closed'},\n",
    "                {'road_id': 'road_22', 'status': 'closed'},\n",
    "                {'road_id': 'road_15', 'status': 'closed'}\n",
    "            ],\n",
    "            'avg_evacuation_time': 105.2,\n",
    "            'probability_high_impact': 0.82,\n",
    "            'coverage': 0.15\n",
    "        }\n",
    "    ]\n",
    "    \n",
    "    # Save to JSON\n",
    "    import json\n",
    "    with open('scenario_discovery_results.json', 'w') as f:\n",
    "        json.dump(scenarios, f, indent=4)\n",
    "    \n",
    "    print(\"Scenario discovery results saved to 'scenario_discovery_results.json'\")\n",
    "\n",
    "generate_scenario_discovery_results()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Road network information saved to 'road_network.csv'\n"
     ]
    }
   ],
   "source": [
    "def generate_road_network_info():\n",
    "    # This would contain information about your road network\n",
    "    # For visualization purposes\n",
    "    \n",
    "    roads = []\n",
    "    \n",
    "    # Generate some random roads with coordinates\n",
    "    # In reality, this would be your actual road network\n",
    "    for i in range(2200):\n",
    "        road = {\n",
    "            'road_id': f'road_{i}',\n",
    "            'start_x': random.uniform(0, 100),\n",
    "            'start_y': random.uniform(0, 100),\n",
    "            'end_x': random.uniform(0, 100),\n",
    "            'end_y': random.uniform(0, 100),\n",
    "            'capacity': random.randint(500, 2000),  # vehicles per hour\n",
    "            'road_type': random.choice(['arterial', 'collector', 'local']),\n",
    "            'zone': random.choice(['north', 'south', 'east', 'west', 'central'])\n",
    "        }\n",
    "        roads.append(road)\n",
    "    \n",
    "    # Save to CSV\n",
    "    road_df = pd.DataFrame(roads)\n",
    "    road_df.to_csv('road_network.csv', index=False)\n",
    "    \n",
    "    print(\"Road network information saved to 'road_network.csv'\")\n",
    "\n",
    "generate_road_network_info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "### evacScenarioFile, roadImportanceFile, scenarioDiscoveryFile, roadNetworkFile\n",
    "def load_data():\n",
    "    \"\"\"\n",
    "    Load all necessary data files for the evacuation visualization tool\n",
    "    \n",
    "    Returns:\n",
    "        df: DataFrame containing road closure scenarios\n",
    "        road_importances: Dictionary mapping road IDs to their importance scores\n",
    "        scenario_rules: List of discovered scenario patterns\n",
    "        road_network: DataFrame containing road network information\n",
    "    \"\"\"\n",
    "    # Load scenario data\n",
    "    try:\n",
    "        #df = pd.read_csv('evacuation_scenarios_sample.csv')\n",
    "        df = pd.read_csv('metaModel/ladris_df.csv')\n",
    "        print(f\"Loaded {len(df)} scenario records with {len(df.columns)-1} road features\")\n",
    "    except FileNotFoundError:\n",
    "        print(\"Warning: Evacuation scenarios file not found. Creating sample data...\")\n",
    "        # Create a small sample dataset if file doesn't exist\n",
    "        num_roads = 50  # Using a small subset for demonstration\n",
    "        columns = [f'road_{i}' for i in range(num_roads)]\n",
    "        columns.append('evacuation_time')\n",
    "        \n",
    "        df = pd.DataFrame(columns=columns)\n",
    "        for i in range(df.columns):  # 100 sample scenarios\n",
    "            # Generate random road closures (1 = closed, 0 = open)\n",
    "            road_closures = np.zeros(num_roads)\n",
    "            num_closed = np.random.randint(2, 10)\n",
    "            closed_indices = np.random.choice(num_roads, num_closed, replace=False)\n",
    "            road_closures[closed_indices] = 1\n",
    "            \n",
    "            # Generate evacuation time (simple model: base time + effect of closures)\n",
    "            base_time = 60\n",
    "            # More effect for lower-numbered roads (assumed to be more important)\n",
    "            importance_weights = np.exp(-np.arange(num_roads)/10)\n",
    "            evacuation_time = base_time + np.sum(road_closures * importance_weights * 20)\n",
    "            evacuation_time += np.random.normal(0, 5)  # Add noise\n",
    "            \n",
    "            row = list(road_closures) + [evacuation_time]\n",
    "            df.loc[i] = row\n",
    "    \n",
    "    # Load feature importance data\n",
    "    try:\n",
    "        #importance_df = pd.read_csv('road_importance.csv')\n",
    "        importance_df = pd.read_csv('metaModel/ladris_feature_importance.csv')\n",
    "        road_importances = dict(zip(importance_df['road_id'], importance_df['importance']))\n",
    "        print(f\"Loaded importance scores for {len(road_importances)} roads\")\n",
    "    except FileNotFoundError:\n",
    "        print(\"Warning: Road importance file not found. Generating from scenario data...\")\n",
    "        # If feature importance file doesn't exist, calculate it from the scenario data\n",
    "        # using a simple Random Forest model\n",
    "        from sklearn.ensemble import RandomForestRegressor\n",
    "        \n",
    "        X = df.drop('evacuation_time', axis=1)\n",
    "        y = df['evacuation_time']\n",
    "        \n",
    "        model = RandomForestRegressor(n_estimators=50, random_state=42)\n",
    "        model.fit(X, y)\n",
    "        \n",
    "        importances = model.feature_importances_\n",
    "        road_importances = {f'road_{i}': imp for i, imp in enumerate(importances)}\n",
    "\n",
    "    ### Load ladris_conv2_scenarioDiscovery.json\n",
    "    try:\n",
    "        #with open('scenario_discovery_results.json', 'r') as f:\n",
    "        with open('metaModel/ladris_conv2_scenarioDiscovery.json', 'r') as f:\n",
    "            scenario_rules = json.load(f)\n",
    "        print(f\"Loaded {len(scenario_rules)} scenario patterns\")\n",
    "    except FileNotFoundError:\n",
    "        print(\"Warning: Scenario discovery file not found. Creating sample scenarios...\")\n",
    "\n",
    "    X_augmented = pd.concat([df, generate_box_features(df, scenario_rules)], axis=1)\n",
    "    \n",
    "    # Load scenario discovery results\n",
    "    try:\n",
    "        #with open('scenario_discovery_results.json', 'r') as f:\n",
    "        with open('metaModel/ladris_scenario_discovery.json', 'r') as f:\n",
    "            scenario_rules = json.load(f)\n",
    "        print(f\"Loaded {len(scenario_rules)} scenario patterns\")\n",
    "    except FileNotFoundError:\n",
    "        print(\"Warning: Scenario discovery file not found. Creating sample scenarios...\")\n",
    "        # Create some sample scenario rules if file doesn't exist\n",
    "        scenario_rules = [\n",
    "            {\n",
    "                'scenario_id': 1,\n",
    "                'description': 'Major arterial routes blocked',\n",
    "                'conditions': [\n",
    "                    {'road_id': 'road_0', 'status': 'closed'},\n",
    "                    {'road_id': 'road_1', 'status': 'closed'}\n",
    "                ],\n",
    "                'avg_evacuation_time': 95.0,\n",
    "                'probability_high_impact': 0.85,\n",
    "                'coverage': 0.25\n",
    "            },\n",
    "            {\n",
    "                'scenario_id': 2,\n",
    "                'description': 'Northern exit routes blocked',\n",
    "                'conditions': [\n",
    "                    {'road_id': 'road_5', 'status': 'closed'},\n",
    "                    {'road_id': 'road_10', 'status': 'closed'}\n",
    "                ],\n",
    "                'avg_evacuation_time': 88.5,\n",
    "                'probability_high_impact': 0.75,\n",
    "                'coverage': 0.20\n",
    "            }\n",
    "        ]\n",
    "    \n",
    "    # Load road network information\n",
    "    try:\n",
    "        #road_network = pd.read_csv('road_network.csv')\n",
    "        road_network = pd.read_csv('metaModel/ladris_latlong.csv')\n",
    "        print(f\"Loaded network data for {len(road_network)} roads\")\n",
    "    except FileNotFoundError:\n",
    "        print(\"Warning: Road network file not found. Creating sample network...\")\n",
    "        # Create a sample road network if file doesn't exist\n",
    "        road_network = pd.DataFrame(columns=[\n",
    "            'road_id', 'start_x', 'start_y', 'end_x', 'end_y', \n",
    "            'capacity', 'road_type', 'zone'\n",
    "        ])\n",
    "        \n",
    "        # Get the road IDs from the scenario data\n",
    "        road_ids = [col for col in df.columns if col.startswith('road_')]\n",
    "        \n",
    "        # Create a grid-like network\n",
    "        grid_size = int(np.sqrt(len(road_ids)))\n",
    "        for i, road_id in enumerate(road_ids):\n",
    "            # Determine road position in grid\n",
    "            row = i // grid_size\n",
    "            col = i % grid_size\n",
    "            \n",
    "            # Determine if horizontal or vertical road\n",
    "            is_horizontal = (i % 2 == 0)\n",
    "            \n",
    "            if is_horizontal:\n",
    "                start_x = col * 10\n",
    "                start_y = row * 10\n",
    "                end_x = (col + 1) * 10\n",
    "                end_y = row * 10\n",
    "            else:\n",
    "                start_x = col * 10\n",
    "                start_y = row * 10\n",
    "                end_x = col * 10\n",
    "                end_y = (row + 1) * 10\n",
    "            \n",
    "            # Assign road type based on position (central roads more important)\n",
    "            center = grid_size // 2\n",
    "            dist_from_center = max(abs(row - center), abs(col - center))\n",
    "            if dist_from_center <= grid_size // 5:\n",
    "                road_type = 'arterial'\n",
    "                capacity = np.random.randint(1500, 2000)\n",
    "            elif dist_from_center <= grid_size // 3:\n",
    "                road_type = 'collector'\n",
    "                capacity = np.random.randint(1000, 1500)\n",
    "            else:\n",
    "                road_type = 'local'\n",
    "                capacity = np.random.randint(500, 1000)\n",
    "            \n",
    "            # Assign zone based on position\n",
    "            if row < grid_size // 2 and col < grid_size // 2:\n",
    "                zone = 'northwest'\n",
    "            elif row < grid_size // 2 and col >= grid_size // 2:\n",
    "                zone = 'northeast'\n",
    "            elif row >= grid_size // 2 and col < grid_size // 2:\n",
    "                zone = 'southwest'\n",
    "            else:\n",
    "                zone = 'southeast'\n",
    "            \n",
    "            # Add to dataframe\n",
    "            road_network.loc[i] = [\n",
    "                road_id, start_x, start_y, end_x, end_y,\n",
    "                capacity, road_type, zone\n",
    "            ]\n",
    "    \n",
    "    return X_augmented, road_importances, scenario_rules, road_network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data...\n",
      "Loaded 500 scenario records with 50 road features\n",
      "Loaded importance scores for 2200 roads\n",
      "Loaded 3 scenario patterns\n",
      "Loaded network data for 2200 roads\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import json\n",
    "\n",
    "# Load your data and build the model\n",
    "print(\"Loading data...\")\n",
    "df, road_importances, scenario_rules, road_network = load_data()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building model...\n"
     ]
    }
   ],
   "source": [
    "print(\"Building model...\")\n",
    "model = build_meta_model(df)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating dashboard...\n"
     ]
    }
   ],
   "source": [
    "print(\"Creating dashboard...\")\n",
    "app = create_dashboard(model, road_importances, scenario_rules, road_network)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running dashboard...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"100%\"\n",
       "            height=\"650\"\n",
       "            src=\"http://127.0.0.1:8050/\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "            \n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.IFrame at 0x7aa1191b57b0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------------------------------------------------\n",
      "AttributeError                            Traceback (most recent call last)\n",
      "Cell In[29], line 73, in create_dashboard.<locals>.update_output(\n",
      "    n_clicks=1,\n",
      "    selected_roads=['road_660', 'road_1967', 'road_387', 'road_606']\n",
      ")\n",
      "     70         X_input[road_idx] = 1\n",
      "     72 # Predict evacuation time\n",
      "---> 73 pred_time = model.predict([X_input])[0]\n",
      "        X_input = array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "        [X_input] = [array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])]\n",
      "        model = (RandomForestRegressor(random_state=42), {'road_0': 0.008931960911302361, 'road_1': 0.03421249039437567, 'road_2': 0.015902702845704397, 'road_3': 0.001661063038296241, 'road_4': 0.0, 'road_5': 0.024306526488900932, 'road_6': 0.027857734687737373, 'road_7': 0.0007408049605910334, 'road_8': 0.007755497624349636, 'road_9': 0.004039009375122657, 'road_10': 0.09161714213147758, 'road_11': 0.006460741732770183, 'road_12': 0.03505999019663065, 'road_13': 0.0017486874516962985, 'road_14': 0.05862414485273971, 'road_15': 0.017259667400729397, 'road_16': 0.0083449550158736, 'road_17': 0.07377114481008348, 'road_18': 0.014349434168091304, 'road_19': 0.014995189842076031, 'road_20': 0.013370244334262852, 'road_21': 0.0, 'road_22': 0.010739376509585005, 'road_23': 0.013241725711249593, 'road_24': 0.08244873677992008, 'road_25': 0.007958127003188835, 'road_26': 0.03232904875570405, 'road_27': 0.03199118342327762, 'road_28': 0.0, 'road_29': 0.021160253913843012, 'road_30': 0.012876846995614453, 'road_31': 0.0, 'road_32': 0.038693800687940746, 'road_33': 0.013322191423329936, 'road_34': 0.02087131922638125, 'road_35': 0.011922911193558778, 'road_36': 0.0021279230295603935, 'road_37': 0.008225837263178881, 'road_38': 0.002962721052634689, 'road_39': 0.0, 'road_40': 0.06055492294270478, 'road_41': 0.026825925298380196, 'road_42': 0.023201327026354288, 'road_43': 0.01447148273310924, 'road_44': 0.012221683257325984, 'road_45': 0.04835972622184263, 'road_46': 0.026027072234292816, 'road_47': 0.011356800315698681, 'road_48': 0.002304248882635446, 'road_49': 0.0027956758558770347})\n",
      "     75 # Create importance graph\n",
      "     76 selected_importances = {road: road_importances.get(road, 0) for road in selected_roads}\n",
      "\n",
      "AttributeError: 'tuple' object has no attribute 'predict'\n",
      "\n",
      "---------------------------------------------------------------------------\n",
      "AttributeError                            Traceback (most recent call last)\n",
      "Cell In[29], line 73, in create_dashboard.<locals>.update_output(\n",
      "    n_clicks=1,\n",
      "    selected_roads=['road_969', 'road_1967', 'road_1124']\n",
      ")\n",
      "     70         X_input[road_idx] = 1\n",
      "     72 # Predict evacuation time\n",
      "---> 73 pred_time = model.predict([X_input])[0]\n",
      "        X_input = array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "        [X_input] = [array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])]\n",
      "        model = (RandomForestRegressor(random_state=42), {'road_0': 0.008931960911302361, 'road_1': 0.03421249039437567, 'road_2': 0.015902702845704397, 'road_3': 0.001661063038296241, 'road_4': 0.0, 'road_5': 0.024306526488900932, 'road_6': 0.027857734687737373, 'road_7': 0.0007408049605910334, 'road_8': 0.007755497624349636, 'road_9': 0.004039009375122657, 'road_10': 0.09161714213147758, 'road_11': 0.006460741732770183, 'road_12': 0.03505999019663065, 'road_13': 0.0017486874516962985, 'road_14': 0.05862414485273971, 'road_15': 0.017259667400729397, 'road_16': 0.0083449550158736, 'road_17': 0.07377114481008348, 'road_18': 0.014349434168091304, 'road_19': 0.014995189842076031, 'road_20': 0.013370244334262852, 'road_21': 0.0, 'road_22': 0.010739376509585005, 'road_23': 0.013241725711249593, 'road_24': 0.08244873677992008, 'road_25': 0.007958127003188835, 'road_26': 0.03232904875570405, 'road_27': 0.03199118342327762, 'road_28': 0.0, 'road_29': 0.021160253913843012, 'road_30': 0.012876846995614453, 'road_31': 0.0, 'road_32': 0.038693800687940746, 'road_33': 0.013322191423329936, 'road_34': 0.02087131922638125, 'road_35': 0.011922911193558778, 'road_36': 0.0021279230295603935, 'road_37': 0.008225837263178881, 'road_38': 0.002962721052634689, 'road_39': 0.0, 'road_40': 0.06055492294270478, 'road_41': 0.026825925298380196, 'road_42': 0.023201327026354288, 'road_43': 0.01447148273310924, 'road_44': 0.012221683257325984, 'road_45': 0.04835972622184263, 'road_46': 0.026027072234292816, 'road_47': 0.011356800315698681, 'road_48': 0.002304248882635446, 'road_49': 0.0027956758558770347})\n",
      "     75 # Create importance graph\n",
      "     76 selected_importances = {road: road_importances.get(road, 0) for road in selected_roads}\n",
      "\n",
      "AttributeError: 'tuple' object has no attribute 'predict'\n",
      "\n",
      "---------------------------------------------------------------------------\n",
      "AttributeError                            Traceback (most recent call last)\n",
      "Cell In[29], line 73, in create_dashboard.<locals>.update_output(\n",
      "    n_clicks=2,\n",
      "    selected_roads=['road_969']\n",
      ")\n",
      "     70         X_input[road_idx] = 1\n",
      "     72 # Predict evacuation time\n",
      "---> 73 pred_time = model.predict([X_input])[0]\n",
      "        X_input = array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "        [X_input] = [array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])]\n",
      "        model = (RandomForestRegressor(random_state=42), {'road_0': 0.008931960911302361, 'road_1': 0.03421249039437567, 'road_2': 0.015902702845704397, 'road_3': 0.001661063038296241, 'road_4': 0.0, 'road_5': 0.024306526488900932, 'road_6': 0.027857734687737373, 'road_7': 0.0007408049605910334, 'road_8': 0.007755497624349636, 'road_9': 0.004039009375122657, 'road_10': 0.09161714213147758, 'road_11': 0.006460741732770183, 'road_12': 0.03505999019663065, 'road_13': 0.0017486874516962985, 'road_14': 0.05862414485273971, 'road_15': 0.017259667400729397, 'road_16': 0.0083449550158736, 'road_17': 0.07377114481008348, 'road_18': 0.014349434168091304, 'road_19': 0.014995189842076031, 'road_20': 0.013370244334262852, 'road_21': 0.0, 'road_22': 0.010739376509585005, 'road_23': 0.013241725711249593, 'road_24': 0.08244873677992008, 'road_25': 0.007958127003188835, 'road_26': 0.03232904875570405, 'road_27': 0.03199118342327762, 'road_28': 0.0, 'road_29': 0.021160253913843012, 'road_30': 0.012876846995614453, 'road_31': 0.0, 'road_32': 0.038693800687940746, 'road_33': 0.013322191423329936, 'road_34': 0.02087131922638125, 'road_35': 0.011922911193558778, 'road_36': 0.0021279230295603935, 'road_37': 0.008225837263178881, 'road_38': 0.002962721052634689, 'road_39': 0.0, 'road_40': 0.06055492294270478, 'road_41': 0.026825925298380196, 'road_42': 0.023201327026354288, 'road_43': 0.01447148273310924, 'road_44': 0.012221683257325984, 'road_45': 0.04835972622184263, 'road_46': 0.026027072234292816, 'road_47': 0.011356800315698681, 'road_48': 0.002304248882635446, 'road_49': 0.0027956758558770347})\n",
      "     75 # Create importance graph\n",
      "     76 selected_importances = {road: road_importances.get(road, 0) for road in selected_roads}\n",
      "\n",
      "AttributeError: 'tuple' object has no attribute 'predict'\n",
      "\n",
      "---------------------------------------------------------------------------\n",
      "AttributeError                            Traceback (most recent call last)\n",
      "Cell In[29], line 73, in create_dashboard.<locals>.update_output(\n",
      "    n_clicks=3,\n",
      "    selected_roads=['road_969']\n",
      ")\n",
      "     70         X_input[road_idx] = 1\n",
      "     72 # Predict evacuation time\n",
      "---> 73 pred_time = model.predict([X_input])[0]\n",
      "        X_input = array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "        [X_input] = [array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])]\n",
      "        model = (RandomForestRegressor(random_state=42), {'road_0': 0.008931960911302361, 'road_1': 0.03421249039437567, 'road_2': 0.015902702845704397, 'road_3': 0.001661063038296241, 'road_4': 0.0, 'road_5': 0.024306526488900932, 'road_6': 0.027857734687737373, 'road_7': 0.0007408049605910334, 'road_8': 0.007755497624349636, 'road_9': 0.004039009375122657, 'road_10': 0.09161714213147758, 'road_11': 0.006460741732770183, 'road_12': 0.03505999019663065, 'road_13': 0.0017486874516962985, 'road_14': 0.05862414485273971, 'road_15': 0.017259667400729397, 'road_16': 0.0083449550158736, 'road_17': 0.07377114481008348, 'road_18': 0.014349434168091304, 'road_19': 0.014995189842076031, 'road_20': 0.013370244334262852, 'road_21': 0.0, 'road_22': 0.010739376509585005, 'road_23': 0.013241725711249593, 'road_24': 0.08244873677992008, 'road_25': 0.007958127003188835, 'road_26': 0.03232904875570405, 'road_27': 0.03199118342327762, 'road_28': 0.0, 'road_29': 0.021160253913843012, 'road_30': 0.012876846995614453, 'road_31': 0.0, 'road_32': 0.038693800687940746, 'road_33': 0.013322191423329936, 'road_34': 0.02087131922638125, 'road_35': 0.011922911193558778, 'road_36': 0.0021279230295603935, 'road_37': 0.008225837263178881, 'road_38': 0.002962721052634689, 'road_39': 0.0, 'road_40': 0.06055492294270478, 'road_41': 0.026825925298380196, 'road_42': 0.023201327026354288, 'road_43': 0.01447148273310924, 'road_44': 0.012221683257325984, 'road_45': 0.04835972622184263, 'road_46': 0.026027072234292816, 'road_47': 0.011356800315698681, 'road_48': 0.002304248882635446, 'road_49': 0.0027956758558770347})\n",
      "     75 # Create importance graph\n",
      "     76 selected_importances = {road: road_importances.get(road, 0) for road in selected_roads}\n",
      "\n",
      "AttributeError: 'tuple' object has no attribute 'predict'\n",
      "\n",
      "---------------------------------------------------------------------------\n",
      "AttributeError                            Traceback (most recent call last)\n",
      "Cell In[29], line 73, in create_dashboard.<locals>.update_output(\n",
      "    n_clicks=1,\n",
      "    selected_roads=['road_969']\n",
      ")\n",
      "     70         X_input[road_idx] = 1\n",
      "     72 # Predict evacuation time\n",
      "---> 73 pred_time = model.predict([X_input])[0]\n",
      "        X_input = array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "        [X_input] = [array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])]\n",
      "        model = (RandomForestRegressor(random_state=42), {'road_0': 0.008931960911302361, 'road_1': 0.03421249039437567, 'road_2': 0.015902702845704397, 'road_3': 0.001661063038296241, 'road_4': 0.0, 'road_5': 0.024306526488900932, 'road_6': 0.027857734687737373, 'road_7': 0.0007408049605910334, 'road_8': 0.007755497624349636, 'road_9': 0.004039009375122657, 'road_10': 0.09161714213147758, 'road_11': 0.006460741732770183, 'road_12': 0.03505999019663065, 'road_13': 0.0017486874516962985, 'road_14': 0.05862414485273971, 'road_15': 0.017259667400729397, 'road_16': 0.0083449550158736, 'road_17': 0.07377114481008348, 'road_18': 0.014349434168091304, 'road_19': 0.014995189842076031, 'road_20': 0.013370244334262852, 'road_21': 0.0, 'road_22': 0.010739376509585005, 'road_23': 0.013241725711249593, 'road_24': 0.08244873677992008, 'road_25': 0.007958127003188835, 'road_26': 0.03232904875570405, 'road_27': 0.03199118342327762, 'road_28': 0.0, 'road_29': 0.021160253913843012, 'road_30': 0.012876846995614453, 'road_31': 0.0, 'road_32': 0.038693800687940746, 'road_33': 0.013322191423329936, 'road_34': 0.02087131922638125, 'road_35': 0.011922911193558778, 'road_36': 0.0021279230295603935, 'road_37': 0.008225837263178881, 'road_38': 0.002962721052634689, 'road_39': 0.0, 'road_40': 0.06055492294270478, 'road_41': 0.026825925298380196, 'road_42': 0.023201327026354288, 'road_43': 0.01447148273310924, 'road_44': 0.012221683257325984, 'road_45': 0.04835972622184263, 'road_46': 0.026027072234292816, 'road_47': 0.011356800315698681, 'road_48': 0.002304248882635446, 'road_49': 0.0027956758558770347})\n",
      "     75 # Create importance graph\n",
      "     76 selected_importances = {road: road_importances.get(road, 0) for road in selected_roads}\n",
      "\n",
      "AttributeError: 'tuple' object has no attribute 'predict'\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Running dashboard...\")\n",
    "app.run(debug=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "import os\n",
    "import signal\n",
    "\n",
    "# Find the process ID\n",
    "# This example works for Linux/Mac\n",
    "# For Windows, you might need a different approach\n",
    "try:\n",
    "    pid = int(os.popen('lsof -i:8050 -t').read().strip())\n",
    "    os.kill(pid, signal.SIGTERM)\n",
    "    print(f\"Killed Dash server process {pid}\")\n",
    "except:\n",
    "    print(\"Could not find or kill the Dash server process\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_improved_meta_model(df):\n",
    "    # Separate features and target\n",
    "    X = df.drop('evacuation_time', axis=1)\n",
    "    y = df['evacuation_time']\n",
    "    \n",
    "    # Add feature engineering - calculate road density/importance metrics\n",
    "    importance_scores = np.zeros(X.shape[1])\n",
    "    for i in range(X.shape[1]):\n",
    "        temp_X = X.copy()\n",
    "        temp_X.iloc[:, i] = 1  # Close this road\n",
    "        model_temp = RandomForestRegressor(n_estimators=10)\n",
    "        model_temp.fit(temp_X, y)\n",
    "        importance_scores[i] = model_temp.feature_importances_[i]\n",
    "    \n",
    "    # Add these feature importance scores as weights\n",
    "    weighted_X = X.copy()\n",
    "    for i in range(X.shape[1]):\n",
    "        weighted_X.iloc[:, i] = X.iloc[:, i] * (importance_scores[i] + 0.1)\n",
    "    \n",
    "    # Train a gradient boosting model instead\n",
    "    from sklearn.ensemble import GradientBoostingRegressor\n",
    "    model = GradientBoostingRegressor(\n",
    "        n_estimators=200,\n",
    "        learning_rate=0.05,\n",
    "        max_depth=4,\n",
    "        min_samples_leaf=3,\n",
    "        random_state=42\n",
    "    )\n",
    "    model.fit(weighted_X, y)\n",
    "    \n",
    "    # Verify model performance\n",
    "    test_scores = cross_val_score(model, weighted_X, y, cv=5)\n",
    "    print(f\"Cross-validation scores: {test_scores.mean():.2f}  {test_scores.std():.2f}\")\n",
    "    \n",
    "    return model, importance_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_improved_prediction(model, importance_scores, input_data):\n",
    "    # Convert input to appropriate format\n",
    "    if isinstance(input_data, pd.DataFrame):\n",
    "        X = input_data.copy()\n",
    "    else:\n",
    "        X = pd.DataFrame([input_data], columns=range(len(input_data)))\n",
    "    \n",
    "    # Apply importance weighting\n",
    "    for i in range(X.shape[1]):\n",
    "        X.iloc[:, i] = X.iloc[:, i] * (importance_scores[i] + 0.1)\n",
    "    \n",
    "    # Make prediction\n",
    "    prediction = model.predict(X)\n",
    "    \n",
    "    # Add baseline travel time (could be data-driven)\n",
    "    baseline = 2.0  # Minimum evacuation time with no closures\n",
    "    \n",
    "    # Scale prediction based on number of closed roads\n",
    "    num_closed = X.sum(axis=1).values[0]\n",
    "    scaling_factor = 1.0 + (num_closed / 100)  # Adjust formula as needed\n",
    "    \n",
    "    return max(baseline, prediction * scaling_factor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data...\n",
      "Loaded 10994 scenario records with 2295 road features\n",
      "Loaded importance scores for 2295 roads\n",
      "Loaded 10 scenario patterns\n",
      "Loaded 1985 scenario patterns\n",
      "Loaded network data for 2295 roads\n",
      "No existing model found. Building a new one...\n",
      "Building model...\n",
      "Model test - Baseline: 3.8616162436849852, Pred1: 3.8577328876127637, Pred2: 3.8651671311545983\n",
      "Model saved as evacuation_model.pkl\n",
      "Creating dashboard...\n",
      "Running dashboard...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/pfoytik/anaconda3/lib/python3.10/site-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but RandomForestRegressor was fitted with feature names\n",
      "  warnings.warn(\n",
      "/home/pfoytik/anaconda3/lib/python3.10/site-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but RandomForestRegressor was fitted with feature names\n",
      "  warnings.warn(\n",
      "/home/pfoytik/anaconda3/lib/python3.10/site-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but RandomForestRegressor was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Main execution\n",
    "if __name__ == '__main__':\n",
    "    print(\"Loading data...\")\n",
    "    ### evacScenarioFile, roadImportanceFile, scenarioDiscoveryFile, roadNetworkFile\n",
    "    df, road_importances, scenario_rules, road_network = load_data()#\"ladris_df.csv\", )\n",
    "    \n",
    "    ### check if evacuation_model.pkl exists\n",
    "    try:\n",
    "        model = joblib.load('metaModel/evacuation_model.pkl')\n",
    "        print(\"Model loaded from evacuation_model.pkl\")\n",
    "    except FileNotFoundError:\n",
    "        print(\"No existing model found. Building a new one...\")\n",
    "        # Build a new model\n",
    "        print(\"Building model...\")\n",
    "        model = build_meta_model(df)    \n",
    "        ### export model\n",
    "        # Save the model to a file    \n",
    "        joblib.dump(model, 'metaModel/evacuation_model.pkl')\n",
    "        print(\"Model saved as evacuation_model.pkl\")\n",
    "\n",
    "    print(\"Creating dashboard...\")\n",
    "    #app = create_dashboard(model, road_importances, scenario_rules, road_network)\n",
    "    \n",
    "    print(\"Running dashboard...\")\n",
    "    #app.run(debug=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "### make a prediction with model\n",
    "def make_prediction(model, input_data):\n",
    "    \"\"\"\n",
    "    Make a prediction using the trained model.\n",
    "    \n",
    "    Parameters:\n",
    "        model: Trained model\n",
    "        input_data: DataFrame containing the input data for prediction\n",
    "    \n",
    "    Returns:\n",
    "        prediction: Predicted evacuation time\n",
    "    \"\"\"\n",
    "    # Ensure input data is in the correct format\n",
    "    if isinstance(input_data, pd.DataFrame):\n",
    "        input_data = input_data.values\n",
    "    \n",
    "    print(input_data)\n",
    "    # Make prediction\n",
    "    prediction = model.predict(input_data)\n",
    "    \n",
    "    return prediction\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature indices: [106, 1081, 1087]\n",
      "Example input:    328559  330552  331652  331795  332555  332843  333389  333943  334433  \\\n",
      "0     0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0   \n",
      "\n",
      "   334569  ...  box_0  box_1  box_2  box_3  box_4  box_5  box_6  box_7  box_8  \\\n",
      "0     0.0  ...    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0   \n",
      "\n",
      "   box_9  \n",
      "0    0.0  \n",
      "\n",
      "[1 rows x 2306 columns]\n",
      "[[0. 0. 0. ... 0. 0. 0.]]\n",
      "Predicted evacuation time: 4.5 hours\n"
     ]
    }
   ],
   "source": [
    "# Example usage\n",
    "road_input = np.zeros(df.shape[1])\n",
    "\n",
    "# Example input data (this should match the feature set used for training)\n",
    "example_input = pd.DataFrame({\n",
    "    'road_0': [1],\n",
    "    'road_1': [0],\n",
    "    'road_2': [1],\n",
    "    # Add more features as needed\n",
    "})\n",
    "\n",
    "### get column index of df for feature 330221, 340133, and 329968\n",
    "feature_indices = [330221, 340133, 329968]\n",
    "feature_indices = [df.columns.get_loc(f'{i}') for i in feature_indices]\n",
    "print(f\"Feature indices: {feature_indices}\")\n",
    "\n",
    "### create an example input where feature 330221, 340133, and 329968 are set to 1\n",
    "example_input = np.zeros(df.shape[1])\n",
    "for idx in feature_indices:\n",
    "    example_input[idx] = 1\n",
    "example_input = pd.DataFrame([example_input], columns=df.columns)\n",
    "print(f\"Example input: {example_input}\")\n",
    "\n",
    "# Make prediction\n",
    "prediction = make_prediction(model, example_input)\n",
    "print(f\"Predicted evacuation time: {prediction[0]} hours\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random indices: [ 805  800  105 2130 1185 1101 1463 2062 1839 2027]\n",
      "[[0. 0. 0. ... 0. 0. 0.]]\n",
      "Test 1: Predicted evacuation time: 3.9114966450995103 hours\n",
      "Random indices: [1594   85    6  369 1539 1735 1319   48  715 2119]\n",
      "[[0. 0. 0. ... 0. 0. 0.]]\n",
      "Test 2: Predicted evacuation time: 3.9462226107715255 hours\n",
      "Random indices: [1034 1185 1573 2285  329  826 1643  778 1735    7]\n",
      "[[0. 0. 0. ... 0. 0. 0.]]\n",
      "Test 3: Predicted evacuation time: 3.871733152601856 hours\n",
      "Random indices: [ 728  299 1385 2001  656 2163 1728 1618  454 1280]\n",
      "[[0. 0. 0. ... 0. 0. 0.]]\n",
      "Test 4: Predicted evacuation time: 3.973831343697361 hours\n",
      "Random indices: [1631  144 2236 1624 1759 1655   40 1644  746 2243]\n",
      "[[0. 0. 0. ... 0. 0. 0.]]\n",
      "Test 5: Predicted evacuation time: 3.9704152857398123 hours\n",
      "Random indices: [ 355 2239 1377 2189 1504 1492  403 1141 2016  559]\n",
      "[[0. 0. 0. ... 0. 0. 0.]]\n",
      "Test 6: Predicted evacuation time: 3.8004829328971397 hours\n",
      "Random indices: [ 320 2043 1881    1 1420 1560 1460  212 1094 1853]\n",
      "[[0. 1. 0. ... 0. 0. 0.]]\n",
      "Test 7: Predicted evacuation time: 3.9535878687642265 hours\n",
      "Random indices: [2195 1071  796 1727 2105 1637  759 1078  586   50]\n",
      "[[0. 0. 0. ... 0. 0. 0.]]\n",
      "Test 8: Predicted evacuation time: 4.0063425023071035 hours\n",
      "Random indices: [1324 2218 1404 1885 1210  334 1894  428 1843  735]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/pfoytik/anaconda3/lib/python3.10/site-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but RandomForestRegressor was fitted with feature names\n",
      "  warnings.warn(\n",
      "/home/pfoytik/anaconda3/lib/python3.10/site-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but RandomForestRegressor was fitted with feature names\n",
      "  warnings.warn(\n",
      "/home/pfoytik/anaconda3/lib/python3.10/site-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but RandomForestRegressor was fitted with feature names\n",
      "  warnings.warn(\n",
      "/home/pfoytik/anaconda3/lib/python3.10/site-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but RandomForestRegressor was fitted with feature names\n",
      "  warnings.warn(\n",
      "/home/pfoytik/anaconda3/lib/python3.10/site-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but RandomForestRegressor was fitted with feature names\n",
      "  warnings.warn(\n",
      "/home/pfoytik/anaconda3/lib/python3.10/site-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but RandomForestRegressor was fitted with feature names\n",
      "  warnings.warn(\n",
      "/home/pfoytik/anaconda3/lib/python3.10/site-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but RandomForestRegressor was fitted with feature names\n",
      "  warnings.warn(\n",
      "/home/pfoytik/anaconda3/lib/python3.10/site-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but RandomForestRegressor was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 0. 0. ... 0. 0. 0.]]\n",
      "Test 9: Predicted evacuation time: 3.96080793138822 hours\n",
      "Random indices: [2151  427 1151 1893 1326  110  414  475  297 1543]\n",
      "[[0. 0. 0. ... 0. 0. 0.]]\n",
      "Test 10: Predicted evacuation time: 3.926831182422479 hours\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/pfoytik/anaconda3/lib/python3.10/site-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but RandomForestRegressor was fitted with feature names\n",
      "  warnings.warn(\n",
      "/home/pfoytik/anaconda3/lib/python3.10/site-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but RandomForestRegressor was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "testDF = df.drop('evacuation_time', axis=1)\n",
    "### Run ten tests with 10 random features set to 1\n",
    "for i in range(10):\n",
    "    # Generate random input with 10 features set to 1\n",
    "    random_input = np.zeros(testDF.shape[1])\n",
    "    random_indices = np.random.choice(testDF.shape[1], 10, replace=False)\n",
    "    ### print the random indices\n",
    "    print(f\"Random indices: {random_indices}\")\n",
    "    for idx in random_indices:\n",
    "        random_input[idx] = 1\n",
    "    random_input = pd.DataFrame([random_input], columns=testDF.columns)\n",
    "    \n",
    "    # Make prediction\n",
    "    prediction = make_prediction(model, random_input)\n",
    "    print(f\"Test {i+1}: Predicted evacuation time: {prediction[0]} hours\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.6429423164238657"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### calculate the standard deviation of df['evacuation_time']\n",
    "df['evacuation_time'].std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<__main__.CustomModel at 0x74f33f2cbfa0>"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
